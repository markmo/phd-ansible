<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

<property>
  <name>dfs.namenode.logging.level</name>
  <value>info</value>
  <description>
    The logging level for dfs namenode. Other values are "dir" (trace
    namespace mutations), "block" (trace block under/over replications
    and block creations/deletions), or "all".
  </description>
</property>

<property>
  <name>dfs.namenode.secondary.http-address</name>
  <value>${secondarynamenode}:50090</value>
  <description>
    The secondary namenode http server address and port.
  </description>
</property>

<property>
  <name>dfs.datanode.address</name>
  <value>0.0.0.0:50010</value>
  <description>
    The datanode server address and port for data transfer.
  </description>
</property>

<property>
  <name>dfs.datanode.http.address</name>
  <value>0.0.0.0:50075</value>
  <description>
    The datanode http server address and port.
  </description>
</property>

<property>
  <name>dfs.datanode.ipc.address</name>
  <value>0.0.0.0:50020</value>
  <description>
    The datanode ipc server address and port.
  </description>
</property>

<property>
  <name>dfs.namenode.http-address</name>
  <value>${namenode}:50070</value>
  <description>
    The address and the base port where the dfs namenode web ui will listen on.
  </description>
</property>

<property>
  <name>dfs.datanode.du.reserved</name>
  <value>0</value>
  <description>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
  </description>
</property>

<property>
  <name>dfs.namenode.name.dir</name>
  <value>${namenode.disk.mount.points}/dfs/name</value>
  <description>Determines where on the local filesystem the DFS name node
      should store the name table(fsimage).  If this is a comma-delimited list
      of directories then the name table is replicated in all of the
      directories, for redundancy. </description>
</property>

<property>
  <name>dfs.permissions.enabled</name>
  <value>true</value>
  <description>
    If "true", enable permission checking in HDFS.
    If "false", permission checking is turned off,
    but all other behavior is unchanged.
    Switching from one parameter value to the other does not change the mode,
    owner or group of files or directories.
  </description>
</property>

<property>
  <name>dfs.permissions.superusergroup</name>
  <value>hadoop</value>
  <description>The name of the group of super-users.</description>
</property>

<property>
  <name>dfs.datanode.data.dir</name>
  <value>${datanode.disk.mount.points}/dfs/data</value>
  <description>Determines where on the local filesystem an DFS data node
  should store its blocks.  If this is a comma-delimited
  list of directories, then data will be stored in all named
  directories, typically on different devices.
  Directories that do not exist are ignored.
  </description>
</property>

<property>
  <name>dfs.replication</name>
  <value>3</value>
  <description>Default block replication.
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>

<property>
  <name>dfs.blocksize</name>
  <value>134217728</value>
  <description>
      The default block size for new files, in bytes.
      You can use the following suffix (case insensitive):
      k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),
      Or provide complete size in bytes (such as 134217728 for 128 MB).
  </description>
</property>

<property>
  <name>dfs.namenode.handler.count</name>
  <value>100</value>
  <description>The number of server threads for the namenode.</description>
</property>

<property>
  <name>dfs.namenode.safemode.threshold-pct</name>
  <value>0.999f</value>
  <description>
    Specifies the percentage of blocks that should satisfy
    the minimal replication requirement defined by dfs.namenode.replication.min.
    Values less than or equal to 0 mean not to wait for any particular
    percentage of blocks before exiting safemode.
    Values greater than 1 will make safe mode permanent.
  </description>
</property>

<property>
  <name>dfs.hosts</name>
  <value></value>
  <description>Names a file that contains a list of hosts that are
  permitted to connect to the namenode. The full pathname of the file
  must be specified.  If the value is empty, all hosts are
  permitted.</description>
</property>

<property>
  <name>dfs.namenode.checkpoint.dir</name>
  <value>${secondary.namenode.disk.mount.points}/dfs/namesecondary</value>
  <description>Determines where on the local filesystem the DFS secondary
      name node should store the temporary images to merge.
      If this is a comma-delimited list of directories then the image is
      replicated in all of the directories for redundancy.
  </description>
</property>

<property>
  <name>dfs.namenode.checkpoint.period</name>
  <value>86400</value>
  <description>The number of seconds between two periodic checkpoints.
  </description>
</property>

<property>
  <name>dfs.webhdfs.enabled</name>
  <value>true</value>
  <description>
    Enable WebHDFS (REST API) in Namenodes and Datanodes.
    ** It is used by GPHD Manager / HAWQ. Keep value to "true"
  </description>
</property>

<property>
  <name>dfs.datanode.max.transfer.threads</name>
  <value>40960</value>
  <description>
    Required for Hbase
  </description>
</property>

<property>
  <name>dfs.namenode.plugins</name>
  <value>org.apache.hadoop.hdfs.server.namenode.GPHDNameNodePlugin</value>
  <description>
    Comma-separated list of namenode plug-ins to be activated.
  </description>
</property>

<property>
  <name>dfs.support.append</name>
  <value>true</value>
  <description>
    Required for Hawq and Hbase
  </description>
</property>

<property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
    <description>
      Bypass hdfs to directly read from local filesystem. Required for Hawq
    </description>
</property>

<property>
    <name>dfs.domain.socket.path</name>
    <value>/var/lib/hadoop-hdfs/dn_socket</value>
</property>

<property>
    <name>dfs.datanode.data.dir.perm</name>
    <value>770</value>
    <description>
      Permissions for the directories on on the local filesystem
      where the DFS data node store its blocks.
      Required for Hawq
    </description>
</property>

<property>
    <name>dfs.hosts.exclude</name>
    <value>/etc/gphd/hadoop/conf/dfs.exclude</value>
   <description>Path to file with datanodes to exclude.</description>
</property>

<!-- HAWQ Specific Property Overrides -->
<property>
    <name>dfs.client.socket-timeout</name>
    <value>300000000</value>
</property>
<property>
    <name>ipc.server.handler.queue.size</name>
    <value>3300</value>
</property>
<property>
    <name>dfs.datanode.handler.count</name>
    <value>60</value>
</property>
<property>
    <name>dfs.namenode.accesstime.precision</name>
    <value>-1</value>
</property>
<property>
    <name>dfs.datanode.socket.write.timeout</name>
    <value>7200000</value>
</property>
<!-- End HAWQ Properties -->
<property>
    <name>dfs.stream-buffer-size</name>
    <value>131072</value>
    <description>The size of buffer to stream files. The size of this buffer should probably be a multiple of hardware page
            size (4096 on Intel x86), and it determines how much data is buffered during read and write operations.
    </description>
</property>
<property>
    <name>dfs.datanode.failed.volumes.tolerated</name>
    <value>${dfs.datanode.failed.volumes.tolerated}</value>
    <description>The number of volumes that are allowed to fail before a datanode stops offering service. By default any
            volume failure will cause a datanode to shutdown.
    </description>
</property>

<!-- Namenode HA related properties -->
<!--
<property>
  <name>dfs.nameservices</name>
  <value>${nameservices}</value>
</property>
<property>
  <name>dfs.ha.namenodes.${nameservices}</name>
  <value>${namenode1id},${namenode2id}</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.${nameservices}.${namenode1id}</name>
  <value>${namenode}:8020</value>
</property>
<property>
  <name>dfs.namenode.rpc-address.${nameservices}.${namenode2id}</name>
  <value>${standbynamenode}:8020</value>
</property>
<property>
  <name>dfs.namenode.http-address.${nameservices}.${namenode1id}</name>
  <value>${namenode}:50070</value>
</property>
<property>
  <name>dfs.namenode.http-address.${nameservices}.${namenode2id}</name>
  <value>${standbynamenode}:50070</value>
</property>
<property>
  <name>dfs.namenode.shared.edits.dir</name>
  <value>qjournal://${journalnode}/${nameservices}</value>
</property>
<property>
  <name>dfs.client.failover.proxy.provider.${nameservices}</name>
  <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
</property>
<property>
  <name>dfs.ha.fencing.methods</name>
  <value>
  sshfence
  shell(/bin/true)
  </value>
</property>
<property>
  <name>dfs.ha.fencing.ssh.private-key-files</name>
  <value>/home/hdfs/.ssh/id_dsa</value>
</property>

<property>
  <name>dfs.journalnode.edits.dir</name>
  <value>${journalpath}</value>
</property>

 <property>
   <name>dfs.ha.automatic-failover.enabled</name>
   <value>true</value>
 </property>
  -->
</configuration>